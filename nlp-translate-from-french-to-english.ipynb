{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-09T02:40:50.815119Z","iopub.execute_input":"2024-03-09T02:40:50.815948Z","iopub.status.idle":"2024-03-09T02:40:51.744945Z","shell.execute_reply.started":"2024-03-09T02:40:50.815914Z","shell.execute_reply":"2024-03-09T02:40:51.743712Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/translate/data/eng-fra.txt\n/kaggle/input/translate/data/names/Vietnamese.txt\n/kaggle/input/translate/data/names/Greek.txt\n/kaggle/input/translate/data/names/Japanese.txt\n/kaggle/input/translate/data/names/Dutch.txt\n/kaggle/input/translate/data/names/Irish.txt\n/kaggle/input/translate/data/names/Russian.txt\n/kaggle/input/translate/data/names/Korean.txt\n/kaggle/input/translate/data/names/Scottish.txt\n/kaggle/input/translate/data/names/Czech.txt\n/kaggle/input/translate/data/names/Italian.txt\n/kaggle/input/translate/data/names/Arabic.txt\n/kaggle/input/translate/data/names/Portuguese.txt\n/kaggle/input/translate/data/names/Spanish.txt\n/kaggle/input/translate/data/names/Chinese.txt\n/kaggle/input/translate/data/names/French.txt\n/kaggle/input/translate/data/names/English.txt\n/kaggle/input/translate/data/names/German.txt\n/kaggle/input/translate/data/names/Polish.txt\n","output_type":"stream"}]},{"cell_type":"markdown","source":"[KEY: > input, = target, < output]\n\n> il est en train de peindre un tableau .\n= he is painting a picture .\n< he is painting a picture .\n\n> pourquoi ne pas essayer ce vin delicieux ?\n= why not try that delicious wine ?\n< why not try that delicious wine ?\n\n> elle n est pas poete mais romanciere .\n= she is not a poet but a novelist .\n< she not not a poet but a novelist .\n\n> vous etes trop maigre .\n= you re too skinny .\n< you re all alone .","metadata":{}},{"cell_type":"code","source":"from __future__ import unicode_literals, print_function, division\nfrom io import open\nimport unicodedata\nimport re\nimport random\n\nimport torch\nimport torch.nn as nn\nfrom torch import optim\nimport torch.nn.functional as F\n\nimport numpy as np\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-03-09T02:40:51.747144Z","iopub.execute_input":"2024-03-09T02:40:51.747704Z","iopub.status.idle":"2024-03-09T02:40:55.390005Z","shell.execute_reply.started":"2024-03-09T02:40:51.747664Z","shell.execute_reply":"2024-03-09T02:40:55.388999Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"SOS_token = 0\nEOS_token = 1\n\nclass Lang:\n    def __init__(self, name):\n        self.name = name\n        self.word2index = {}\n        self.word2count = {}\n        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n        self.n_words = 2  # Count SOS and EOS\n\n    def addSentence(self, sentence):\n        for word in sentence.split(' '):\n            self.addWord(word)\n\n    def addWord(self, word):\n        if word not in self.word2index:\n            self.word2index[word] = self.n_words\n            self.word2count[word] = 1\n            self.index2word[self.n_words] = word\n            self.n_words += 1\n        else:\n            self.word2count[word] += 1","metadata":{"execution":{"iopub.status.busy":"2024-03-09T02:40:55.391729Z","iopub.execute_input":"2024-03-09T02:40:55.392266Z","iopub.status.idle":"2024-03-09T02:40:55.399950Z","shell.execute_reply.started":"2024-03-09T02:40:55.392229Z","shell.execute_reply":"2024-03-09T02:40:55.398880Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Turn a Unicode string to plain ASCII, thanks to\n# https://stackoverflow.com/a/518232/2809427\ndef unicodeToAscii(s):\n    return ''.join(\n        c for c in unicodedata.normalize('NFD', s)\n        if unicodedata.category(c) != 'Mn'\n    )\n\n# Lowercase, trim, and remove non-letter characters\ndef normalizeString(s):\n    s = unicodeToAscii(s.lower().strip())\n    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n    return s.strip()","metadata":{"execution":{"iopub.status.busy":"2024-03-09T02:40:55.403099Z","iopub.execute_input":"2024-03-09T02:40:55.403457Z","iopub.status.idle":"2024-03-09T02:40:55.412204Z","shell.execute_reply.started":"2024-03-09T02:40:55.403424Z","shell.execute_reply":"2024-03-09T02:40:55.411272Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def readLangs(lang1, lang2, reverse=False):\n    print(\"Reading lines...\")\n\n    # Read the file and split into lines\n#     lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n#         read().strip().split('\\n')\n    lines = open('/kaggle/input/translate/data/eng-fra.txt' , encoding='utf-8').\\\n        read().strip().split('\\n')\n\n    # Split every line into pairs and normalize\n    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n\n    # Reverse pairs, make Lang instances\n    if reverse:\n        pairs = [list(reversed(p)) for p in pairs]\n        input_lang = Lang(lang2)\n        output_lang = Lang(lang1)\n    else:\n        input_lang = Lang(lang1)\n        output_lang = Lang(lang2)\n\n    return input_lang, output_lang, pairs","metadata":{"execution":{"iopub.status.busy":"2024-03-09T02:40:55.415156Z","iopub.execute_input":"2024-03-09T02:40:55.415512Z","iopub.status.idle":"2024-03-09T02:40:55.425528Z","shell.execute_reply.started":"2024-03-09T02:40:55.415488Z","shell.execute_reply":"2024-03-09T02:40:55.424703Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"MAX_LENGTH = 10\n\neng_prefixes = (\n    \"i am \", \"i m \",\n    \"he is\", \"he s \",\n    \"she is\", \"she s \",\n    \"you are\", \"you re \",\n    \"we are\", \"we re \",\n    \"they are\", \"they re \"\n)\n\ndef filterPair(p):\n    return len(p[0].split(' ')) < MAX_LENGTH and \\\n        len(p[1].split(' ')) < MAX_LENGTH and \\\n        p[1].startswith(eng_prefixes)\n\n\ndef filterPairs(pairs):\n    return [pair for pair in pairs if filterPair(pair)]","metadata":{"execution":{"iopub.status.busy":"2024-03-09T02:40:55.426544Z","iopub.execute_input":"2024-03-09T02:40:55.426791Z","iopub.status.idle":"2024-03-09T02:40:55.434860Z","shell.execute_reply.started":"2024-03-09T02:40:55.426769Z","shell.execute_reply":"2024-03-09T02:40:55.434019Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def prepareData(lang1, lang2, reverse=False):\n    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n    print(\"Read %s sentence pairs\" % len(pairs))\n    pairs = filterPairs(pairs)\n    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n    print(\"Counting words...\")\n    for pair in pairs:\n        input_lang.addSentence(pair[0])\n        output_lang.addSentence(pair[1])\n    print(\"Counted words:\")\n    print(input_lang.name, input_lang.n_words)\n    print(output_lang.name, output_lang.n_words)\n    return input_lang, output_lang, pairs\n\ninput_lang, output_lang, pairs = prepareData('eng', 'fra', True)\nprint(random.choice(pairs))","metadata":{"execution":{"iopub.status.busy":"2024-03-09T02:40:55.436015Z","iopub.execute_input":"2024-03-09T02:40:55.436328Z","iopub.status.idle":"2024-03-09T02:41:02.310979Z","shell.execute_reply.started":"2024-03-09T02:40:55.436305Z","shell.execute_reply":"2024-03-09T02:41:02.310063Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Reading lines...\nRead 135842 sentence pairs\nTrimmed to 11445 sentence pairs\nCounting words...\nCounted words:\nfra 4601\neng 2991\n['il n est jamais satisfait', 'he s never satisfied']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"A Sequence to Sequence network, or seq2seq network, or Encoder Decoder network, is a model consisting of two RNNs called the encoder and decoder. \n### The encoder reads an input sequence and outputs **a single vector**, \nand the decoder reads that vector to produce **an output sequence**.","metadata":{}},{"cell_type":"code","source":"class EncoderRNN(nn.Module):\n    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n        super(EncoderRNN, self).__init__()\n        self.hidden_size = hidden_size\n\n        self.embedding = nn.Embedding(input_size, hidden_size)\n        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n        self.dropout = nn.Dropout(dropout_p)\n\n    def forward(self, input):\n        embedded = self.dropout(self.embedding(input))\n        output, hidden = self.gru(embedded)\n        return output, hidden","metadata":{"execution":{"iopub.status.busy":"2024-03-09T02:41:02.312131Z","iopub.execute_input":"2024-03-09T02:41:02.312474Z","iopub.status.idle":"2024-03-09T02:41:02.319238Z","shell.execute_reply.started":"2024-03-09T02:41:02.312447Z","shell.execute_reply":"2024-03-09T02:41:02.318080Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Method 1: GRU Decoder","metadata":{}},{"cell_type":"code","source":"class DecoderRNN(nn.Module):\n    def __init__(self, hidden_size, output_size):\n        super(DecoderRNN, self).__init__()\n        self.embedding = nn.Embedding(output_size, hidden_size)\n        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n        self.out = nn.Linear(hidden_size, output_size)\n\n    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n        batch_size = encoder_outputs.size(0)\n        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n        decoder_hidden = encoder_hidden\n        decoder_outputs = []\n\n        for i in range(MAX_LENGTH):\n            decoder_output, decoder_hidden  = self.forward_step(decoder_input, decoder_hidden)\n            decoder_outputs.append(decoder_output)\n\n            if target_tensor is not None:\n                # Teacher forcing: Feed the target as the next input\n                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n            else:\n                # Without teacher forcing: use its own predictions as the next input\n                _, topi = decoder_output.topk(1)\n                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n\n        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n        return decoder_outputs, decoder_hidden, None # We return `None` for consistency in the training loop\n\n    def forward_step(self, input, hidden):\n        output = self.embedding(input)\n        output = F.relu(output)\n        output, hidden = self.gru(output, hidden)\n        output = self.out(output)\n        return output, hidden","metadata":{"execution":{"iopub.status.busy":"2024-03-09T02:41:02.320324Z","iopub.execute_input":"2024-03-09T02:41:02.320589Z","iopub.status.idle":"2024-03-09T02:41:02.331819Z","shell.execute_reply.started":"2024-03-09T02:41:02.320567Z","shell.execute_reply":"2024-03-09T02:41:02.331035Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Method 2: Attention Decoder\nwe will be using Bahdanau attention. However, it would be a valuable exercise to explore modifying the attention mechanism to use Luong attention.","metadata":{}},{"cell_type":"code","source":"class BahdanauAttention(nn.Module):\n    def __init__(self, hidden_size):\n        super(BahdanauAttention, self).__init__()\n        self.Wa = nn.Linear(hidden_size, hidden_size)\n        self.Ua = nn.Linear(hidden_size, hidden_size)\n        self.Va = nn.Linear(hidden_size, 1)\n\n    def forward(self, query, keys):\n        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n        scores = scores.squeeze(2).unsqueeze(1)\n\n        weights = F.softmax(scores, dim=-1)\n        context = torch.bmm(weights, keys)\n\n        return context, weights\n\nclass AttnDecoderRNN(nn.Module):\n    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n        super(AttnDecoderRNN, self).__init__()\n        self.embedding = nn.Embedding(output_size, hidden_size)\n        self.attention = BahdanauAttention(hidden_size)\n        self.gru = nn.GRU(2 * hidden_size, hidden_size, batch_first=True)\n        self.out = nn.Linear(hidden_size, output_size)\n        self.dropout = nn.Dropout(dropout_p)\n\n    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n        batch_size = encoder_outputs.size(0)\n        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n        decoder_hidden = encoder_hidden\n        decoder_outputs = []\n        attentions = []\n\n        for i in range(MAX_LENGTH):\n            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n                decoder_input, decoder_hidden, encoder_outputs\n            )\n            decoder_outputs.append(decoder_output)\n            attentions.append(attn_weights)\n\n            if target_tensor is not None:\n                # Teacher forcing: Feed the target as the next input\n                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n            else:\n                # Without teacher forcing: use its own predictions as the next input\n                _, topi = decoder_output.topk(1)\n                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n\n        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n        attentions = torch.cat(attentions, dim=1)\n\n        return decoder_outputs, decoder_hidden, attentions\n\n\n    def forward_step(self, input, hidden, encoder_outputs):\n        embedded =  self.dropout(self.embedding(input))\n\n        query = hidden.permute(1, 0, 2)\n        context, attn_weights = self.attention(query, encoder_outputs)\n        input_gru = torch.cat((embedded, context), dim=2)\n\n        output, hidden = self.gru(input_gru, hidden)\n        output = self.out(output)\n\n        return output, hidden, attn_weights","metadata":{"execution":{"iopub.status.busy":"2024-03-09T02:41:02.335649Z","iopub.execute_input":"2024-03-09T02:41:02.336006Z","iopub.status.idle":"2024-03-09T02:41:02.351144Z","shell.execute_reply.started":"2024-03-09T02:41:02.335974Z","shell.execute_reply":"2024-03-09T02:41:02.350368Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Training\nPreparing Training Data","metadata":{}},{"cell_type":"code","source":"def indexesFromSentence(lang, sentence):\n    return [lang.word2index[word] for word in sentence.split(' ')]\n\ndef tensorFromSentence(lang, sentence):\n    indexes = indexesFromSentence(lang, sentence)\n    indexes.append(EOS_token)\n    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n\ndef tensorsFromPair(pair):\n    input_tensor = tensorFromSentence(input_lang, pair[0])\n    target_tensor = tensorFromSentence(output_lang, pair[1])\n    return (input_tensor, target_tensor)\n\ndef get_dataloader(batch_size):\n    input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n\n    n = len(pairs)\n    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n\n    for idx, (inp, tgt) in enumerate(pairs):\n        inp_ids = indexesFromSentence(input_lang, inp)\n        tgt_ids = indexesFromSentence(output_lang, tgt)\n        inp_ids.append(EOS_token)\n        tgt_ids.append(EOS_token)\n        input_ids[idx, :len(inp_ids)] = inp_ids\n        target_ids[idx, :len(tgt_ids)] = tgt_ids\n\n    train_data = TensorDataset(torch.LongTensor(input_ids).to(device),\n                               torch.LongTensor(target_ids).to(device))\n\n    train_sampler = RandomSampler(train_data)\n    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n    return input_lang, output_lang, train_dataloader","metadata":{"execution":{"iopub.status.busy":"2024-03-09T02:41:02.352122Z","iopub.execute_input":"2024-03-09T02:41:02.352390Z","iopub.status.idle":"2024-03-09T02:41:02.366491Z","shell.execute_reply.started":"2024-03-09T02:41:02.352368Z","shell.execute_reply":"2024-03-09T02:41:02.365446Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### Training the Model","metadata":{}},{"cell_type":"code","source":"def train_epoch(dataloader, encoder, decoder, encoder_optimizer,\n          decoder_optimizer, criterion):\n\n    total_loss = 0\n    for data in dataloader:\n        input_tensor, target_tensor = data\n\n        encoder_optimizer.zero_grad()\n        decoder_optimizer.zero_grad()\n\n        encoder_outputs, encoder_hidden = encoder(input_tensor)\n        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n\n        loss = criterion(\n            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n            target_tensor.view(-1)\n        )\n        loss.backward()\n\n        encoder_optimizer.step()\n        decoder_optimizer.step()\n\n        total_loss += loss.item()\n\n    return total_loss / len(dataloader)","metadata":{"execution":{"iopub.status.busy":"2024-03-09T02:41:02.367616Z","iopub.execute_input":"2024-03-09T02:41:02.367893Z","iopub.status.idle":"2024-03-09T02:41:02.379171Z","shell.execute_reply.started":"2024-03-09T02:41:02.367870Z","shell.execute_reply":"2024-03-09T02:41:02.378351Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"This is a helper function to print time elapsed and estimated time remaining given the current time and progress %","metadata":{}},{"cell_type":"code","source":"import time\nimport math\n\ndef asMinutes(s):\n    m = math.floor(s / 60)\n    s -= m * 60\n    return '%dm %ds' % (m, s)\n\ndef timeSince(since, percent):\n    now = time.time()\n    s = now - since\n    es = s / (percent)\n    rs = es - s\n    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))","metadata":{"execution":{"iopub.status.busy":"2024-03-09T02:41:02.380336Z","iopub.execute_input":"2024-03-09T02:41:02.380613Z","iopub.status.idle":"2024-03-09T02:41:02.391723Z","shell.execute_reply.started":"2024-03-09T02:41:02.380591Z","shell.execute_reply":"2024-03-09T02:41:02.390927Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def train(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001,\n               print_every=100, plot_every=100):\n    start = time.time()\n    plot_losses = []\n    print_loss_total = 0  # Reset every print_every\n    plot_loss_total = 0  # Reset every plot_every\n\n    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n    criterion = nn.NLLLoss()\n\n    for epoch in range(1, n_epochs + 1):\n        loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n        print_loss_total += loss\n        plot_loss_total += loss\n\n        if epoch % print_every == 0:\n            print_loss_avg = print_loss_total / print_every\n            print_loss_total = 0\n            print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs),\n                                        epoch, epoch / n_epochs * 100, print_loss_avg))\n\n        if epoch % plot_every == 0:\n            plot_loss_avg = plot_loss_total / plot_every\n            plot_losses.append(plot_loss_avg)\n            plot_loss_total = 0\n\n    showPlot(plot_losses)","metadata":{"execution":{"iopub.status.busy":"2024-03-09T02:41:02.392799Z","iopub.execute_input":"2024-03-09T02:41:02.393073Z","iopub.status.idle":"2024-03-09T02:41:02.402157Z","shell.execute_reply.started":"2024-03-09T02:41:02.393050Z","shell.execute_reply":"2024-03-09T02:41:02.401371Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"Plotting results\nPlotting is done with matplotlib, using the array of loss values plot_losses saved while training.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.switch_backend('agg')\nimport matplotlib.ticker as ticker\nimport numpy as np\n\ndef showPlot(points):\n    plt.figure()\n    fig, ax = plt.subplots()\n    # this locator puts ticks at regular intervals\n    loc = ticker.MultipleLocator(base=0.2)\n    ax.yaxis.set_major_locator(loc)\n    plt.plot(points)","metadata":{"execution":{"iopub.status.busy":"2024-03-09T02:41:02.403388Z","iopub.execute_input":"2024-03-09T02:41:02.403626Z","iopub.status.idle":"2024-03-09T02:41:02.414674Z","shell.execute_reply.started":"2024-03-09T02:41:02.403606Z","shell.execute_reply":"2024-03-09T02:41:02.413887Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"### Evaluation\nEvaluation is mostly the same as training, but there are no targets so we simply feed the decoder’s predictions back to itself for each step. Every time it predicts a word we add it to the output string, and if it predicts the EOS token we stop there. We also store the decoder’s attention outputs for display later.","metadata":{}},{"cell_type":"code","source":"def evaluate(encoder, decoder, sentence, input_lang, output_lang):\n    with torch.no_grad():\n        input_tensor = tensorFromSentence(input_lang, sentence)\n\n        encoder_outputs, encoder_hidden = encoder(input_tensor)\n        decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n\n        _, topi = decoder_outputs.topk(1)\n        decoded_ids = topi.squeeze()\n\n        decoded_words = []\n        for idx in decoded_ids:\n            if idx.item() == EOS_token:\n                decoded_words.append('<EOS>')\n                break\n            decoded_words.append(output_lang.index2word[idx.item()])\n    return decoded_words, decoder_attn","metadata":{"execution":{"iopub.status.busy":"2024-03-09T02:41:02.415674Z","iopub.execute_input":"2024-03-09T02:41:02.415990Z","iopub.status.idle":"2024-03-09T02:41:02.423720Z","shell.execute_reply.started":"2024-03-09T02:41:02.415960Z","shell.execute_reply":"2024-03-09T02:41:02.422888Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"We can evaluate random sentences from the training set and print out the input, target, and output to make some subjective quality judgements:","metadata":{}},{"cell_type":"code","source":"def evaluateRandomly(encoder, decoder, n=10):\n    for i in range(n):\n        pair = random.choice(pairs)\n        print('>', pair[0])\n        print('=', pair[1])\n        output_words, _ = evaluate(encoder, decoder, pair[0], input_lang, output_lang)\n        output_sentence = ' '.join(output_words)\n        print('<', output_sentence)\n        print('')","metadata":{"execution":{"iopub.status.busy":"2024-03-09T02:41:02.424753Z","iopub.execute_input":"2024-03-09T02:41:02.425613Z","iopub.status.idle":"2024-03-09T02:41:02.437281Z","shell.execute_reply.started":"2024-03-09T02:41:02.425589Z","shell.execute_reply":"2024-03-09T02:41:02.436423Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"hidden_size = 128\nbatch_size = 32\n\ninput_lang, output_lang, train_dataloader = get_dataloader(batch_size)\n\nencoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\ndecoder = AttnDecoderRNN(hidden_size, output_lang.n_words).to(device)\n\ntrain(train_dataloader, encoder, decoder, 320, print_every=5, plot_every=5)","metadata":{"execution":{"iopub.status.busy":"2024-03-09T02:55:25.219754Z","iopub.execute_input":"2024-03-09T02:55:25.220370Z","iopub.status.idle":"2024-03-09T03:29:41.743391Z","shell.execute_reply.started":"2024-03-09T02:55:25.220337Z","shell.execute_reply":"2024-03-09T03:29:41.740038Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Reading lines...\nRead 135842 sentence pairs\nTrimmed to 11445 sentence pairs\nCounting words...\nCounted words:\nfra 4601\neng 2991\n0m 32s (- 33m 41s) (5 1%) 1.5189\n1m 4s (- 33m 4s) (10 3%) 0.6742\n1m 36s (- 32m 32s) (15 4%) 0.3502\n2m 8s (- 32m 1s) (20 6%) 0.1949\n2m 40s (- 31m 29s) (25 7%) 0.1214\n3m 12s (- 30m 56s) (30 9%) 0.0843\n3m 43s (- 30m 23s) (35 10%) 0.0642\n4m 15s (- 29m 50s) (40 12%) 0.0522\n4m 48s (- 29m 21s) (45 14%) 0.0450\n5m 20s (- 28m 48s) (50 15%) 0.0401\n5m 52s (- 28m 16s) (55 17%) 0.0372\n6m 24s (- 27m 45s) (60 18%) 0.0351\n6m 56s (- 27m 12s) (65 20%) 0.0330\n7m 28s (- 26m 40s) (70 21%) 0.0311\n8m 0s (- 26m 9s) (75 23%) 0.0301\n8m 32s (- 25m 37s) (80 25%) 0.0296\n9m 4s (- 25m 4s) (85 26%) 0.0281\n9m 36s (- 24m 33s) (90 28%) 0.0274\n10m 8s (- 24m 0s) (95 29%) 0.0267\n10m 40s (- 23m 28s) (100 31%) 0.0260\n11m 12s (- 22m 56s) (105 32%) 0.0259\n11m 43s (- 22m 23s) (110 34%) 0.0254\n12m 15s (- 21m 51s) (115 35%) 0.0249\n12m 47s (- 21m 19s) (120 37%) 0.0246\n13m 19s (- 20m 47s) (125 39%) 0.0242\n13m 51s (- 20m 15s) (130 40%) 0.0236\n14m 23s (- 19m 43s) (135 42%) 0.0239\n14m 55s (- 19m 11s) (140 43%) 0.0236\n15m 27s (- 18m 38s) (145 45%) 0.0229\n15m 58s (- 18m 6s) (150 46%) 0.0224\n16m 30s (- 17m 34s) (155 48%) 0.0227\n17m 2s (- 17m 2s) (160 50%) 0.0223\n17m 34s (- 16m 31s) (165 51%) 0.0225\n18m 7s (- 15m 59s) (170 53%) 0.0220\n18m 41s (- 15m 28s) (175 54%) 0.0224\n19m 13s (- 14m 56s) (180 56%) 0.0219\n19m 45s (- 14m 25s) (185 57%) 0.0217\n20m 17s (- 13m 53s) (190 59%) 0.0224\n20m 49s (- 13m 20s) (195 60%) 0.0212\n21m 21s (- 12m 48s) (200 62%) 0.0215\n21m 53s (- 12m 16s) (205 64%) 0.0214\n22m 25s (- 11m 44s) (210 65%) 0.0213\n22m 57s (- 11m 12s) (215 67%) 0.0212\n23m 29s (- 10m 40s) (220 68%) 0.0210\n24m 0s (- 10m 8s) (225 70%) 0.0210\n24m 32s (- 9m 36s) (230 71%) 0.0209\n25m 4s (- 9m 4s) (235 73%) 0.0212\n25m 36s (- 8m 32s) (240 75%) 0.0205\n26m 8s (- 8m 0s) (245 76%) 0.0206\n26m 40s (- 7m 28s) (250 78%) 0.0210\n27m 12s (- 6m 56s) (255 79%) 0.0204\n27m 44s (- 6m 24s) (260 81%) 0.0202\n28m 16s (- 5m 52s) (265 82%) 0.0209\n28m 48s (- 5m 20s) (270 84%) 0.0204\n29m 20s (- 4m 48s) (275 85%) 0.0202\n29m 52s (- 4m 16s) (280 87%) 0.0204\n30m 24s (- 3m 44s) (285 89%) 0.0199\n30m 56s (- 3m 12s) (290 90%) 0.0204\n31m 28s (- 2m 40s) (295 92%) 0.0201\n32m 1s (- 2m 8s) (300 93%) 0.0203\n32m 33s (- 1m 36s) (305 95%) 0.0202\n33m 5s (- 1m 4s) (310 96%) 0.0197\n33m 37s (- 0m 32s) (315 98%) 0.0205\n34m 9s (- 0m 0s) (320 100%) 0.0202\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Set dropout layers to eval mode","metadata":{}},{"cell_type":"code","source":"encoder.eval()\ndecoder.eval()\nevaluateRandomly(encoder, decoder)","metadata":{"execution":{"iopub.status.busy":"2024-03-09T02:49:47.884698Z","iopub.execute_input":"2024-03-09T02:49:47.885459Z","iopub.status.idle":"2024-03-09T02:49:47.990026Z","shell.execute_reply.started":"2024-03-09T02:49:47.885410Z","shell.execute_reply":"2024-03-09T02:49:47.989117Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"> vous etes a sec\n= you re out of booze\n< you re out of booze <EOS>\n\n> je suis desole de vous appeler au travail\n= i m sorry i m calling you at work\n< i m sorry i m calling you at work <EOS>\n\n> je suis le plus grand de notre classe\n= i am the tallest in our class\n< i am the tallest in our class in class <EOS>\n\n> tu n es pas contrariee si ?\n= you re not upset are you ?\n< you re not upset are you ? <EOS>\n\n> vous etes plutot bonne\n= you re pretty good\n< you re pretty good friends <EOS>\n\n> il n est pas vraiment un artiste\n= he is not much of an artist\n< he is not much of an artist <EOS>\n\n> vous etes tres timides\n= you re very timid\n< you re very timid <EOS>\n\n> je ne vais pas prendre parti\n= i m not taking sides\n< i m not taking sides <EOS>\n\n> on peut te remplacer\n= you re replaceable\n< you re replaceable <EOS>\n\n> je suis egalement heureux\n= i m happy too\n< i m happy too <EOS>\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## VISUALIZE ATTENTION","metadata":{}},{"cell_type":"code","source":"def showAttention(input_sentence, output_words, attentions):\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    cax = ax.matshow(attentions.cpu().numpy(), cmap='bone')\n    fig.colorbar(cax)\n\n    # Set up axes\n    ax.set_xticklabels([''] + input_sentence.split(' ') +\n                       ['<EOS>'], rotation=90)\n    ax.set_yticklabels([''] + output_words)\n\n    # Show label at every tick\n    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n\n    plt.show()\n\n\ndef evaluateAndShowAttention(input_sentence):\n    output_words, attentions = evaluate(encoder, decoder, input_sentence, input_lang, output_lang)\n    print('input =', input_sentence)\n    print('output =', ' '.join(output_words))\n    showAttention(input_sentence, output_words, attentions[0, :len(output_words), :])\n\n\nevaluateAndShowAttention('il n est pas aussi grand que son pere')\n\nevaluateAndShowAttention('je suis trop fatigue pour conduire')\n\nevaluateAndShowAttention('je suis desole si c est une question idiote')\n\nevaluateAndShowAttention('je suis reellement fiere de vous')","metadata":{"execution":{"iopub.status.busy":"2024-03-09T02:49:47.991353Z","iopub.execute_input":"2024-03-09T02:49:47.991776Z","iopub.status.idle":"2024-03-09T02:49:48.940280Z","shell.execute_reply.started":"2024-03-09T02:49:47.991744Z","shell.execute_reply":"2024-03-09T02:49:48.938788Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"input = il n est pas aussi grand que son pere\noutput = he is not as tall as his father <EOS>\ninput = je suis trop fatigue pour conduire\noutput = i m too tired to drive <EOS>\ninput = je suis desole si c est une question idiote\noutput = i m sorry if this is a stupid question <EOS>\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_34/1690937169.py:8: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax.set_xticklabels([''] + input_sentence.split(' ') +\n/tmp/ipykernel_34/1690937169.py:10: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax.set_yticklabels([''] + output_words)\n","output_type":"stream"},{"name":"stdout","text":"input = je suis reellement fiere de vous\noutput = i m really proud of you <EOS>\n","output_type":"stream"}]}]}